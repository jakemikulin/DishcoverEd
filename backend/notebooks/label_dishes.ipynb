{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify recipe ingredients to cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from https://www.kaggle.com/competitions/whats-cooking/\n",
    "\n",
    "Solution based on https://www.kaggle.com/code/dipayan/whatscooking-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combination of Word lemmatization + LinearSVC model finally pushes the accuracy score past 80%\n",
    "\n",
    "traindf = pd.read_json(\"../labels/train.json\")\n",
    "traindf['ingredients_clean_string'] = [' , '.join(z).strip() for z in traindf['ingredients']]  \n",
    "traindf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in traindf['ingredients']]       \n",
    "\n",
    "testdf = pd.read_json(\"../labels/test.json\") \n",
    "testdf['ingredients_clean_string'] = [' , '.join(z).strip() for z in testdf['ingredients']]\n",
    "testdf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in testdf['ingredients']]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize training data\n",
    "corpustr = traindf['ingredients_string']\n",
    "vectorizertr = TfidfVectorizer(stop_words='english',\n",
    "                               ngram_range=(1, 1), analyzer=\"word\",\n",
    "                               max_df=0.57, binary=False,\n",
    "                               token_pattern=r'\\w+', sublinear_tf=False)\n",
    "tfidftr = vectorizertr.fit_transform(corpustr)  # Keep sparse\n",
    "\n",
    "# Vectorize test data\n",
    "corpusts = testdf['ingredients_string']\n",
    "tfidfts = vectorizertr.transform(corpusts)  # Keep sparse\n",
    "\n",
    "predictors_tr = tfidftr\n",
    "predictors_ts = tfidfts\n",
    "targets_tr = traindf['cuisine']\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)  # Increase iterations\n",
    "clf.fit(predictors_tr, targets_tr)  # No scaling needed\n",
    "\n",
    "# Predict on test set\n",
    "predictions = clf.predict(predictors_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf['cuisine'] = predictions\n",
    "# testdf = testdf.sort_values('id' , ascending=True)\n",
    "\n",
    "# testdf[['id' , 'ingredients_clean_string' , 'cuisine' ]].to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Recipes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 rows...\n",
      "Processed 20000 rows...\n",
      "Processed 30000 rows...\n",
      "Processed 40000 rows...\n",
      "Processed 50000 rows...\n",
      "Processed 60000 rows...\n",
      "Processed 70000 rows...\n",
      "Processed 80000 rows...\n",
      "Processed 90000 rows...\n",
      "Processed 100000 rows...\n",
      "Processed 110000 rows...\n",
      "Processed 120000 rows...\n",
      "Processed 130000 rows...\n",
      "Processed 140000 rows...\n",
      "Processed 150000 rows...\n",
      "Processed 160000 rows...\n",
      "Processed 170000 rows...\n",
      "Processed 180000 rows...\n",
      "Processed 190000 rows...\n",
      "Processed 200000 rows...\n",
      "Processed 210000 rows...\n",
      "Processed 220000 rows...\n",
      "Processed 230000 rows...\n",
      "Processed 240000 rows...\n",
      "Processed 250000 rows...\n",
      "Processed 260000 rows...\n",
      "Processed 270000 rows...\n",
      "Processed 280000 rows...\n",
      "Processed 290000 rows...\n",
      "Processed 300000 rows...\n",
      "Processed 310000 rows...\n",
      "Processed 320000 rows...\n",
      "Processed 330000 rows...\n",
      "Processed 340000 rows...\n",
      "Processed 350000 rows...\n",
      "Processed 360000 rows...\n",
      "Processed 370000 rows...\n",
      "Processed 380000 rows...\n",
      "Processed 390000 rows...\n",
      "Processed 400000 rows...\n",
      "Processed 410000 rows...\n",
      "Processed 420000 rows...\n",
      "Processed 430000 rows...\n",
      "Processed 440000 rows...\n",
      "Processed 450000 rows...\n",
      "Processed 460000 rows...\n",
      "Processed 470000 rows...\n",
      "Processed 480000 rows...\n",
      "Processed 490000 rows...\n",
      "Processed 500000 rows...\n",
      "Processed 510000 rows...\n",
      "Processed 520000 rows...\n",
      "Processed 530000 rows...\n",
      "Processed 540000 rows...\n",
      "Processed 550000 rows...\n",
      "Processed 560000 rows...\n",
      "Processed 570000 rows...\n",
      "Processed 580000 rows...\n",
      "Processed 590000 rows...\n",
      "Processed 600000 rows...\n",
      "Processed 610000 rows...\n",
      "Processed 620000 rows...\n",
      "Processed 630000 rows...\n",
      "Processed 640000 rows...\n",
      "Processed 650000 rows...\n",
      "Processed 660000 rows...\n",
      "Processed 670000 rows...\n",
      "Processed 680000 rows...\n",
      "Processed 690000 rows...\n",
      "Processed 700000 rows...\n",
      "Processed 710000 rows...\n",
      "Processed 720000 rows...\n",
      "Processed 730000 rows...\n",
      "Processed 740000 rows...\n",
      "Processed 750000 rows...\n",
      "Processed 760000 rows...\n",
      "Processed 770000 rows...\n",
      "Processed 780000 rows...\n",
      "Processed 790000 rows...\n",
      "Processed 800000 rows...\n",
      "Processed 810000 rows...\n",
      "Processed 820000 rows...\n",
      "Processed 830000 rows...\n",
      "Processed 840000 rows...\n",
      "Processed 850000 rows...\n",
      "Processed 860000 rows...\n",
      "Processed 870000 rows...\n",
      "Processed 880000 rows...\n",
      "Processed 890000 rows...\n",
      "Processed 900000 rows...\n",
      "Processed 910000 rows...\n",
      "Processed 920000 rows...\n",
      "Processed 930000 rows...\n",
      "Processed 940000 rows...\n",
      "Processed 950000 rows...\n",
      "Processed 960000 rows...\n",
      "Processed 970000 rows...\n",
      "Processed 980000 rows...\n",
      "Processed 990000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1010000 rows...\n",
      "Processed 1020000 rows...\n",
      "Processed 1030000 rows...\n",
      "Processed 1040000 rows...\n",
      "Processed 1050000 rows...\n",
      "Processed 1060000 rows...\n",
      "Processed 1070000 rows...\n",
      "Processed 1080000 rows...\n",
      "Processed 1090000 rows...\n",
      "Processed 1100000 rows...\n",
      "Processed 1110000 rows...\n",
      "Processed 1120000 rows...\n",
      "Processed 1130000 rows...\n",
      "Processed 1140000 rows...\n",
      "Processed 1150000 rows...\n",
      "Processed 1160000 rows...\n",
      "Processed 1170000 rows...\n",
      "Processed 1180000 rows...\n",
      "Processed 1190000 rows...\n",
      "Processed 1200000 rows...\n",
      "Processed 1210000 rows...\n",
      "Processed 1220000 rows...\n",
      "Processed 1230000 rows...\n",
      "Processed 1240000 rows...\n",
      "Processed 1250000 rows...\n",
      "Processed 1260000 rows...\n",
      "Processed 1270000 rows...\n",
      "Processed 1280000 rows...\n",
      "Processed 1290000 rows...\n",
      "Processed 1300000 rows...\n",
      "Processed 1310000 rows...\n",
      "Processed 1320000 rows...\n",
      "Processed 1330000 rows...\n",
      "Processed 1340000 rows...\n",
      "Processed 1350000 rows...\n",
      "Processed 1360000 rows...\n",
      "Processed 1370000 rows...\n",
      "Processed 1380000 rows...\n",
      "Processed 1390000 rows...\n",
      "Processed 1400000 rows...\n",
      "Processed 1410000 rows...\n",
      "Processed 1420000 rows...\n",
      "Processed 1430000 rows...\n",
      "Processed 1440000 rows...\n",
      "Processed 1450000 rows...\n",
      "Processed 1460000 rows...\n",
      "Processed 1470000 rows...\n",
      "Processed 1480000 rows...\n",
      "Processed 1490000 rows...\n",
      "Processed 1500000 rows...\n",
      "Processed 1510000 rows...\n",
      "Processed 1520000 rows...\n",
      "Processed 1530000 rows...\n",
      "Processed 1540000 rows...\n",
      "Processed 1550000 rows...\n",
      "Processed 1560000 rows...\n",
      "Processed 1570000 rows...\n",
      "Processed 1580000 rows...\n",
      "Processed 1590000 rows...\n",
      "Processed 1600000 rows...\n",
      "Processed 1610000 rows...\n",
      "Processed 1620000 rows...\n",
      "Processed 1630000 rows...\n",
      "Processed 1640000 rows...\n",
      "Processed 1650000 rows...\n",
      "Processed 1660000 rows...\n",
      "Processed 1670000 rows...\n",
      "Processed 1680000 rows...\n",
      "Processed 1690000 rows...\n",
      "Processed 1700000 rows...\n",
      "Processed 1710000 rows...\n",
      "Processed 1720000 rows...\n",
      "Processed 1730000 rows...\n",
      "Processed 1740000 rows...\n",
      "Processed 1750000 rows...\n",
      "Processed 1760000 rows...\n",
      "Processed 1770000 rows...\n",
      "Processed 1780000 rows...\n",
      "Processed 1790000 rows...\n",
      "Processed 1800000 rows...\n",
      "Processed 1810000 rows...\n",
      "Processed 1820000 rows...\n",
      "Processed 1830000 rows...\n",
      "Processed 1840000 rows...\n",
      "Processed 1850000 rows...\n",
      "Processed 1860000 rows...\n",
      "Processed 1870000 rows...\n",
      "Processed 1880000 rows...\n",
      "Processed 1890000 rows...\n",
      "Processed 1900000 rows...\n",
      "Processed 1910000 rows...\n",
      "Processed 1920000 rows...\n",
      "Processed 1930000 rows...\n",
      "Processed 1940000 rows...\n",
      "Processed 1950000 rows...\n",
      "Processed 1960000 rows...\n",
      "Processed 1970000 rows...\n",
      "Processed 1980000 rows...\n",
      "Processed 1990000 rows...\n",
      "Processed 2000000 rows...\n",
      "Processed 2010000 rows...\n",
      "Processed 2020000 rows...\n",
      "Processed 2030000 rows...\n",
      "Processed 2040000 rows...\n",
      "Processed 2050000 rows...\n",
      "Processed 2060000 rows...\n",
      "Processed 2070000 rows...\n",
      "Processed 2080000 rows...\n",
      "Processed 2090000 rows...\n",
      "Processed 2100000 rows...\n",
      "Processed 2110000 rows...\n",
      "Processed 2120000 rows...\n",
      "Processed 2130000 rows...\n",
      "Processed 2140000 rows...\n",
      "Processed 2150000 rows...\n",
      "Processed 2160000 rows...\n",
      "Processed 2170000 rows...\n",
      "Processed 2180000 rows...\n",
      "Processed 2190000 rows...\n",
      "Processed 2200000 rows...\n",
      "Processed 2210000 rows...\n",
      "Processed 2220000 rows...\n",
      "Processed 2230000 rows...\n",
      "Processed 2231142 rows...\n",
      "Updated dataset saved as ../recipes_with_cuisines.csv\n"
     ]
    }
   ],
   "source": [
    "# Define chunk size\n",
    "chunk_size = 10000  \n",
    "total_rows = 0  \n",
    "output_file = \"../recipes_with_cuisines.csv\"\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    # Process in chunks\n",
    "    for chunk in pd.read_csv(\"../recipes_with_labels.csv\", chunksize=chunk_size):\n",
    "        # Ensure NER column is properly formatted as lists\n",
    "        chunk['NER'] = chunk['NER'].apply(ast.literal_eval)\n",
    "        \n",
    "        # Convert the NER list to a string for classification\n",
    "        ingredients_text = chunk['NER'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "        # Vectorize using the same vectorizer from training\n",
    "        tfidfts_new = vectorizertr.transform(ingredients_text)\n",
    "\n",
    "        # Predict cuisines\n",
    "        chunk['cuisine'] = clf.predict(tfidfts_new)\n",
    "\n",
    "        # Save chunk to CSV (append after first batch)\n",
    "        chunk.to_csv(output_file, mode='a', header=(total_rows == 0), index=False)\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "        print(f\"Processed {total_rows} rows...\")\n",
    "\n",
    "print(f\"Updated dataset saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      title  \\\n",
      "0       No-Bake Nut Cookies   \n",
      "1     Jewell Ball'S Chicken   \n",
      "2               Creamy Corn   \n",
      "3             Chicken Funny   \n",
      "4      Reeses Cups(Candy)     \n",
      "5  Cheeseburger Potato Soup   \n",
      "6       Rhubarb Coffee Cake   \n",
      "7            Scalloped Corn   \n",
      "8      Nolan'S Pepper Steak   \n",
      "9           Millionaire Pie   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
      "1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
      "2  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
      "3  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
      "4  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
      "5  [\"6 baking potatoes\", \"1 lb. of extra lean gro...   \n",
      "6  [\"1 1/2 c. sugar\", \"1/2 c. butter\", \"1 egg\", \"...   \n",
      "7  [\"1 can cream-style corn\", \"1 can whole kernel...   \n",
      "8  [\"1 1/2 lb. round steak (1-inch thick), cut in...   \n",
      "9  [\"1 large container Cool Whip\", \"1 large can c...   \n",
      "\n",
      "                                          directions  \\\n",
      "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
      "1  [\"Place chipped beef on bottom of baking dish....   \n",
      "2  [\"In a slow cooker, combine all ingredients. C...   \n",
      "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
      "4  [\"Combine first four ingredients and press in ...   \n",
      "5  [\"Wash potatoes; prick several times with a fo...   \n",
      "6  [\"Cream sugar and butter.\", \"Add egg and beat ...   \n",
      "7  [\"Mix together both cans of corn, crackers, eg...   \n",
      "8  [\"Roll steak strips in flour.\", \"Brown in skil...   \n",
      "9  [\"Empty Cool Whip into a bowl.\", \"Drain juice ...   \n",
      "\n",
      "                                              link    source  \\\n",
      "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
      "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
      "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
      "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
      "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
      "5   www.cookbooks.com/Recipe-Details.aspx?id=20115  Gathered   \n",
      "6  www.cookbooks.com/Recipe-Details.aspx?id=210288  Gathered   \n",
      "7  www.cookbooks.com/Recipe-Details.aspx?id=876969  Gathered   \n",
      "8  www.cookbooks.com/Recipe-Details.aspx?id=375254  Gathered   \n",
      "9  www.cookbooks.com/Recipe-Details.aspx?id=794547  Gathered   \n",
      "\n",
      "                                                 NER               site  \\\n",
      "0  ['bite size shredded rice biscuits', 'vanilla'...  www.cookbooks.com   \n",
      "1  ['cream of mushroom soup', 'beef', 'sour cream...  www.cookbooks.com   \n",
      "2  ['frozen corn', 'pepper', 'cream cheese', 'gar...  www.cookbooks.com   \n",
      "3  ['chicken gravy', 'cream of mushroom soup', 'c...  www.cookbooks.com   \n",
      "4  ['graham cracker crumbs', 'powdered sugar', 'p...  www.cookbooks.com   \n",
      "5  ['sour cream', 'bacon', 'pepper', 'extra lean ...  www.cookbooks.com   \n",
      "6  ['buttermilk', 'egg', 'sugar', 'vanilla', 'sod...  www.cookbooks.com   \n",
      "7  ['egg', 'pepper', 'crackers', 'cream-style cor...  www.cookbooks.com   \n",
      "8  ['oil', 'tomatoes', 'green peppers', 'water', ...  www.cookbooks.com   \n",
      "9  ['condensed milk', 'lemons', 'graham cracker c...  www.cookbooks.com   \n",
      "\n",
      "                                          categories      cuisine  \n",
      "0           ['Dairy', 'Additive', 'Fruit', 'Bakery']  southern_us  \n",
      "1                        ['Dairy', 'Meat', 'Fungus']      russian  \n",
      "2    ['Herb', 'Dairy', 'Maize', 'Spice', 'Additive']  southern_us  \n",
      "3                        ['Dairy', 'Meat', 'Fungus']  southern_us  \n",
      "4  ['Legume', 'Beverage Alcoholic', 'Nuts & Seed'...  southern_us  \n",
      "5  ['Vegetable', 'Dairy', 'Meat', 'Spice', 'Addit...  southern_us  \n",
      "6     ['Herb', 'Dairy', 'Meat', 'Fruit', 'Additive']  southern_us  \n",
      "7                ['Dairy', 'Spice', 'Meat', 'Maize']  southern_us  \n",
      "8                 ['Vegetable', 'Spice', 'Additive']      chinese  \n",
      "9  ['Nuts & Seed', 'Dairy', 'Plant', 'Meat', 'Fru...  southern_us  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../recipes_with_cuisines.csv\", nrows=10)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
